{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c108c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"/home/vuiem/KLTN/chroma_db\")\n",
    "collection = client.create_collection(\"concept_cards\")\n",
    "google_ef  = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=\"\")\n",
    "\n",
    "collection = client.create_collection(name=\"kltn_test\", embedding_function=google_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0131d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from uuid import uuid4\n",
    "\n",
    "for week in range(1, 9):\n",
    "\n",
    "    with open(f'/home/vuiem/KLTN/test/concept_cards/concept_cards_{week}.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "        embedding = google_ef(['\\n'.join(item['summary'])])[0]\n",
    "        collection.add(\n",
    "            ids=[str(uuid4())],\n",
    "            embeddings=[embedding],\n",
    "            documents=['\\n'.join(item[\"summary\"])],   # text để search\n",
    "            metadatas=[{\n",
    "                \"name\": item[\"name\"],\n",
    "                \"week\": week,\n",
    "                \"formulae\": '\\n'.join(item[\"formulae\"]),\n",
    "                \"examples\": '\\n'.join(item[\"examples\"]),\n",
    "                \"common_pitfalls\": '\\n'.join(item[\"common_pitfalls\"]),\n",
    "                \"page\": '\\n'.join(map(str, item[\"page\"])),\n",
    "            }]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01124189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['9c620f94-b0d6-4ce6-824c-ff1d97ff4b4a',\n",
       "   '7a88b070-99c7-41a6-a795-7fc7fe074e16',\n",
       "   'c5aaccea-1217-4e72-9e53-78ad9a0cba6b']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Supervised learning (inductive learning) involves learning a function from labeled training data consisting of input-output pairs.\\nThe goal is to approximate an unknown target function f: X -> Y using a hypothesis h: X -> Y, such that h is close to f.\\nInput features are denoted as x (a vector in R^n), and outputs are y. For regression, y is a real number (R); for binary classification, y is {+1, -1}; for multi-class classification, y is {1, 2, ..., K}.\\nThe training data D is a set of m examples: D = {(x_1, y_1), ..., (x_m, y_m)}.\\nThe hypothesis h belongs to a hypothesis space H, which defines the set of possible functions the learning algorithm can choose from (e.g., linear, polynomial, non-linear models).\\nA regression problem specifically deals with predicting a real-valued output.',\n",
       "   \"True Error (or Risk) measures the target performance of a model on unseen data, representing the probability of misclassification for classification tasks or mean squared error for regression tasks.\\nEmpirical Error (or Risk) measures the model's performance on the training data, calculated as the misclassification rate for classification or mean squared error for regression.\\nOverfitting occurs when a model learns the training data too well, including noise, leading to low empirical error but high true error on new, unseen data. This often happens with overly complex models.\\nUnderfitting occurs when a model is too simple to capture the underlying patterns in the training data, resulting in high empirical error and high true error. The model fails to learn the relationships effectively.\\nModel complexity plays a crucial role: low complexity leads to underfitting, while excessively high complexity leads to overfitting. The goal is to find the 'Best model' that minimizes true error by balancing complexity.\",\n",
       "   \"Model optimization aims to find the 'Best model' by addressing overfitting and underfitting, typically by adjusting model complexity.\\nFeature selection is a method to reduce model complexity by identifying and selecting the most relevant features, thereby improving model performance and interpretability.\\nUnsupervised feature selection methods include dropping incomplete features, features with high multicollinearity, or features with near-zero variance.\\nSupervised feature selection methods are categorized into Filters, Wrappers, and Embedded methods.\\nFilter methods evaluate feature importance independently of the learning algorithm (e.g., Information Gain, Chi-square). They are computationally efficient.\\nWrapper methods use a specific learning algorithm to evaluate subsets of features, often involving iterative search strategies like Forward Selection or Backward Elimination. They are more computationally intensive but can find better feature subsets for a given model.\\nEmbedded methods perform feature selection as part of the model training process itself (e.g., LASSO, Auto-encoder with bottleneck).\"]],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'examples': 'Predicting house prices based on size (single variable regression).\\nPredicting house prices based on multiple features like size, number of bedrooms, number of floors, and age (multivariable regression).',\n",
       "    'formulae': 'Input: x in X subset of R^n\\nOutput: y in Y (R for regression, {+1, -1} for binary classification, {1, 2, ..., K} for multi-class classification)\\nTarget Function: f: X -> Y (unknown)\\nTraining Data: D = {(x_1, y_1), ..., (x_m, y_m)}\\nHypothesis: h: X -> Y such that h approx f',\n",
       "    'page': '4\\n5\\n9\\n10\\n11\\n12\\n13\\n14',\n",
       "    'week': 3,\n",
       "    'common_pitfalls': 'Choosing an inappropriate hypothesis space (e.g., linear model for non-linear data).\\nOverfitting: when the hypothesis h fits the training data too well but performs poorly on unseen data.',\n",
       "    'name': 'Supervised Learning and Regression Problem Formulation'},\n",
       "   {'examples': 'Linear Regression for house prices: A simple linear model (w0 + w1x) might underfit, a quadratic model (w0 + w1x + w2x^2) might fit well, while a high-degree polynomial (w0 + w1x + w2x^2 + w3x^3 + w4x^4) might overfit by perfectly fitting training data but failing on new examples.\\nLogistic Regression for classification: A linear decision boundary might underfit, a moderately complex curve might fit well, and a highly complex, winding curve that isolates individual data points indicates overfitting.',\n",
       "    'common_pitfalls': 'Relying solely on empirical error (training error) as an indicator of model performance, as it does not reflect generalization ability.\\nChoosing a model that is either too simple (underfitting) or too complex (overfitting) for the given data and problem.',\n",
       "    'page': '5\\n6\\n7\\n8\\n9\\n10\\n20',\n",
       "    'name': 'True Error vs. Empirical Error, Overfitting, and Underfitting',\n",
       "    'week': 7,\n",
       "    'formulae': 'True Error (Classification): P(f(X) ≠ Y)\\nTrue Error (Regression): E[(f(X) – Y)²]\\nEmpirical Error (Classification): (1/n) Σ_{i=1}^n 1_{f(X_i)≠Y_i}\\nEmpirical Error (Regression): (1/n) Σ_{i=1}^n (f(X_i) - Y_i)²\\nR(f) = E_{XY} [loss(f(X), Y)]\\nR̂(f) = (1/n) Σ_{i=1}^n loss(f(X_i), Y_i)'},\n",
       "   {'formulae': 'Conditional Entropy: E(T, X) = Σ_{c∈X} P(c)E(c)\\nInformation Gain: Gain(T, X) = Entropy(T) - Entropy(T, X)\\nChi-squared: X^2 = Σ_{i=1}^n (O_i - E_i)^2 / E_i',\n",
       "    'week': 7,\n",
       "    'name': 'Model Optimization: Feature Selection',\n",
       "    'page': '22\\n23\\n24\\n25\\n26\\n27\\n28\\n29',\n",
       "    'common_pitfalls': 'Filter methods might select features that are individually good but redundant when combined.\\nWrapper methods can be computationally expensive, especially with a large number of features.\\nOver-reliance on feature selection without considering regularization, which can also manage model complexity.',\n",
       "    'examples': \"Information Gain calculation for 'Play Golf' dataset to determine the best feature (e.g., Outlook) for classification.\\nUsing Chi-squared to select features by calculating the score between each feature and the target, selecting features with the highest scores.\\nForward selection: Start with no features and iteratively add the feature that most improves model performance.\\nBackward elimination: Start with all features and iteratively remove the feature whose removal least harms model performance.\"}]],\n",
       " 'distances': [[0.43177831172943115, 0.43898093700408936, 0.4480120837688446]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Analyze a given dataset scenario, choose an appropriate supervised learning model (e.g., Linear Regression or Logistic Regression), justify the choice, discuss potential issues like overfitting/underfitting, and propose optimization strategies (e.g., regularization).\"\n",
    "\n",
    "embeddings = google_ef([query])[0]\n",
    "results = collection.query(\n",
    "    query_embeddings=[embeddings],\n",
    "    n_results=3,\n",
    "    where={\"week\": {\"$in\": [3, 4, 7]}}\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e58c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
