{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c3dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lite_llm import LiteLLMService, LiteLLMEmbeddingInput, LiteLLMSetting \n",
    "from pydantic import HttpUrl, SecretStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e22cb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLMService(litellm_setting=LiteLLMSetting(url=HttpUrl('http://localhost:9510/'), token=SecretStr('**********'), model='gemini-2.5-flash', frequency_penalty=0.0, n=1, temperature=0.0, top_p=1.0, max_completion_tokens=10000, dimension=1536, embedding_model='gemini-embedding'), async_client=None, client=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm_service=LiteLLMService(\n",
    "    litellm_setting=LiteLLMSetting(\n",
    "        url=HttpUrl(\"http://localhost:9510\"),\n",
    "        token=SecretStr(\"abc123\"),\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        frequency_penalty=0.0,\n",
    "        n=1,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_completion_tokens=10000,\n",
    "        dimension=1536,\n",
    "        embedding_model=\"gemini-embedding\"\n",
    "    )\n",
    ")\n",
    "litellm_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5dd4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await litellm_service.embedding_llm_async(\n",
    "    inputs=LiteLLMEmbeddingInput(\n",
    "        text=\"Hello, world!\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3acbc55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0253c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 embeddings, each with dimension 1536\n"
     ]
    }
   ],
   "source": [
    "options = [\n",
    "    \"By explicitly computing feature vectors in a higher-dimensional space and then applying a linear SVM.\",\n",
    "    \"By replacing the inner product of mapped feature vectors with a kernel function, avoiding explicit computation in high-dimensional space.\",  # correct\n",
    "    \"By directly adjusting the weights `w` and bias `b` to create a non-linear decision boundary in the input space.\",\n",
    "    \"By reducing the dimensionality of the input data before applying a linear SVM.\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get embeddings for each option\n",
    "option_embeddings = []\n",
    "for option in options:\n",
    "    embedding_output = await litellm_service.embedding_llm_async(\n",
    "        inputs=LiteLLMEmbeddingInput(text=option)\n",
    "    )\n",
    "    option_embeddings.append(np.array(embedding_output.embedding))\n",
    "\n",
    "print(f\"Generated {len(option_embeddings)} embeddings, each with dimension {len(option_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63e17509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def cosine_similarity(vector1: np.ndarray, vector2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vector1: First vector as numpy array\n",
    "        vector2: Second vector as numpy array\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity value between -1 and 1\n",
    "    \"\"\"\n",
    "    # Calculate dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Calculate magnitudes (L2 norms)\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def compute_similarity_matrix(texts: List[str], embeddings: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a similarity matrix for a list of text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        embeddings: List of embedding vectors\n",
    "    \n",
    "    Returns:\n",
    "        NxN similarity matrix where N is the number of texts\n",
    "    \"\"\"\n",
    "    n = len(embeddings)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ab60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4 Cosine Similarity Matrix:\n",
      "==================================================\n",
      "Option 1: 1.0000  0.8497  0.6905  0.7976  \n",
      "Option 2: 0.8497  1.0000  0.6641  0.7329  \n",
      "Option 3: 0.6905  0.6641  1.0000  0.6729  \n",
      "Option 4: 0.7976  0.7329  0.6729  1.0000  \n",
      "\n",
      "Detailed Matrix with Labels:\n",
      "================================================================================\n",
      "\n",
      "Option 1: By explicitly computing feature vectors in a highe...\n",
      "  vs Option 1: 1.0000\n",
      "  vs Option 2: 0.8497\n",
      "  vs Option 3: 0.6905\n",
      "  vs Option 4: 0.7976\n",
      "\n",
      "Option 2: By replacing the inner product of mapped feature v...\n",
      "  vs Option 1: 0.8497\n",
      "  vs Option 2: 1.0000\n",
      "  vs Option 3: 0.6641\n",
      "  vs Option 4: 0.7329\n",
      "\n",
      "Option 3: By directly adjusting the weights `w` and bias `b`...\n",
      "  vs Option 1: 0.6905\n",
      "  vs Option 2: 0.6641\n",
      "  vs Option 3: 1.0000\n",
      "  vs Option 4: 0.6729\n",
      "\n",
      "Option 4: By reducing the dimensionality of the input data b...\n",
      "  vs Option 1: 0.7976\n",
      "  vs Option 2: 0.7329\n",
      "  vs Option 3: 0.6729\n",
      "  vs Option 4: 1.0000\n",
      "\n",
      "Numpy Array:\n",
      "[[1.         0.84968432 0.69050761 0.79757521]\n",
      " [0.84968432 1.         0.66406493 0.73291624]\n",
      " [0.69050761 0.66406493 1.         0.67288202]\n",
      " [0.79757521 0.73291624 0.67288202 1.        ]]\n",
      "\n",
      "Matrix Shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Compute the 4x4 similarity matrix\n",
    "similarity_matrix = compute_similarity_matrix(options, option_embeddings)\n",
    "\n",
    "print(\"4x4 Cosine Similarity Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print matrix with proper formatting\n",
    "for i in range(len(options)):\n",
    "    row_str = \"\"\n",
    "    for j in range(len(options)):\n",
    "        row_str += f\"{similarity_matrix[i, j]:.4f}  \"\n",
    "    print(f\"Option {i+1}: {row_str}\")\n",
    "\n",
    "print(\"\\nDetailed Matrix with Labels:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print with option labels for clarity\n",
    "for i, option1 in enumerate(options):\n",
    "    print(f\"\\nOption {i+1}: {option1[:50]}...\")\n",
    "    for j, option2 in enumerate(options):\n",
    "        print(f\"  vs Option {j+1}: {similarity_matrix[i, j]:.4f}\")\n",
    "\n",
    "# Also display as a clean numpy array\n",
    "print(f\"\\nNumpy Array:\\n{similarity_matrix}\")\n",
    "print(f\"\\nMatrix Shape: {similarity_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74af856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from lite_llm import LiteLLMEmbeddingInput, LiteLLMService, LiteLLMSetting\n",
    "from pydantic import HttpUrl, SecretStr\n",
    "\n",
    "litellm_setting=LiteLLMSetting(\n",
    "    url=HttpUrl(\"http://localhost:9510\"),\n",
    "    token=SecretStr(\"abc123\"),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    frequency_penalty=0.0,\n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_completion_tokens=10000,\n",
    "    dimension=1536,\n",
    "    embedding_model=\"gemini-embedding\"\n",
    ")\n",
    "\n",
    "litellm_service = LiteLLMService(litellm_setting=litellm_setting)\n",
    "client = chromadb.PersistentClient(path=\"./chroma_database\")\n",
    "collection = client.get_or_create_collection(name=\"questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1d98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [03:41<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "\n",
    "with open(\"/home/vuiem/KLTN/services/generation/src/generation/shared/static_files/ml_mcqs.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in tqdm(data):\n",
    "    embedding = await litellm_service.embedding_llm_async(\n",
    "        inputs=LiteLLMEmbeddingInput(\n",
    "            text=item['question']\n",
    "        )\n",
    "    )\n",
    "    options = [\n",
    "        f\"{key}. {value}\"\n",
    "        for key, value in item['options'].items()\n",
    "    ]\n",
    "    collection.add(\n",
    "        ids=[str(uuid4())],\n",
    "        embeddings=[embedding.embedding],\n",
    "        documents=[item['question']],   # text để search\n",
    "        metadatas=[{\n",
    "            \"options\": \"\\n\".join(options),\n",
    "            \"answer\": item['answer'],\n",
    "            \"explanation\": item['explanation'],\n",
    "        }]\n",
    "    )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8727082f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['ba055faa-e1ee-43ad-9647-836899649535',\n",
       "   'd273bde6-37e8-4a11-a4ca-afcdddf3a405',\n",
       "   '21a1fef9-e19b-426a-82a0-2fd80bf4ee78']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['What is the role of the margin in SVM?',\n",
       "   'What is the main purpose of a Support Vector Machine (SVM)?',\n",
       "   'What is a support vector in SVM?']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'options': 'A. To separate the training and test data\\nB. To measure the distance between support vectors\\nC. To maximize the distance between data points and the hyperplane\\nD. To reduce model complexity',\n",
       "    'explanation': 'SVM aims to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, improving generalization.',\n",
       "    'answer': 'To maximize the distance between data points and the hyperplane'},\n",
       "   {'answer': 'To classify data points',\n",
       "    'explanation': 'SVM is primarily used for classification tasks by finding the hyperplane that best separates data points into different classes.',\n",
       "    'options': 'A. To reduce the number of features\\nB. To classify data points\\nC. To increase data size\\nD. To perform clustering'},\n",
       "   {'answer': 'A data point closest to the hyperplane',\n",
       "    'explanation': 'Support vectors are the data points that are closest to the hyperplane and directly influence its position and orientation.',\n",
       "    'options': 'A. A data point farthest from the hyperplane\\nB. A data point closest to the hyperplane\\nC. A misclassified data point\\nD. A point used for outlier detection'}]],\n",
       " 'distances': [[0.3072980046272278, 0.32094597816467285, 0.33811286091804504]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hard Margin SVM Optimization Problem\"\n",
    "\n",
    "embeddings = await litellm_service.embedding_llm_async(\n",
    "    inputs=LiteLLMEmbeddingInput(\n",
    "        text=query\n",
    "    )\n",
    ")\n",
    "results = collection.query(\n",
    "    query_embeddings=[embeddings.embedding],\n",
    "    n_results=3,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2cf2cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the role of the margin in SVM?',\n",
       " 'What is the main purpose of a Support Vector Machine (SVM)?',\n",
       " 'What is a support vector in SVM?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941ce8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'options': 'A. To separate the training and test data\\nB. To measure the distance between support vectors\\nC. To maximize the distance between data points and the hyperplane\\nD. To reduce model complexity',\n",
       "  'explanation': 'SVM aims to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, improving generalization.',\n",
       "  'answer': 'To maximize the distance between data points and the hyperplane'},\n",
       " {'answer': 'To classify data points',\n",
       "  'explanation': 'SVM is primarily used for classification tasks by finding the hyperplane that best separates data points into different classes.',\n",
       "  'options': 'A. To reduce the number of features\\nB. To classify data points\\nC. To increase data size\\nD. To perform clustering'},\n",
       " {'answer': 'A data point closest to the hyperplane',\n",
       "  'explanation': 'Support vectors are the data points that are closest to the hyperplane and directly influence its position and orientation.',\n",
       "  'options': 'A. A data point farthest from the hyperplane\\nB. A data point closest to the hyperplane\\nC. A misclassified data point\\nD. A point used for outlier detection'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fb176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    13\n",
      "1    31\n",
      "2     4\n",
      "0     4\n",
      "1     5\n",
      "2     6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s1 = pd.Series([13, 31, 4])\n",
    "s2 = pd.Series([4, 5, 6])\n",
    "\n",
    "# Concatenate vertically (stack one after another)\n",
    "result = pd.concat([s1, s2])\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
