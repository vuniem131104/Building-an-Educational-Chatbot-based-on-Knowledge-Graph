{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28f9112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLMOutput(response=Questions(questions=[Question(question='What is the primary intuition behind Support Vector Machines (SVM) for finding the optimal decision boundary?', answer='Maximize the margin between the separating hyperplane and the closest data points from both classes', distractors=['Minimize the total number of misclassified training examples', 'Maximize the likelihood of the training data given the model parameters', 'Minimize the entropy of the class distributions at the decision boundary'], explanation=\"SVM's core intuition is to find the separating hyperplane that maximizes the margin - the width that the boundary could be increased by before hitting a data point. This approach aims to find the decision boundary that is as far as possible from the nearest training examples of both classes, which typically leads to better generalization.\"), Question(question='In the soft margin SVM formulation, what role does the regularization parameter C play?', answer='It controls the trade-off between maximizing the margin and minimizing misclassification errors', distractors=['It determines the degree of the polynomial kernel function', 'It sets the bandwidth parameter for the Gaussian RBF kernel', 'It controls the learning rate for the gradient descent optimization'], explanation='The parameter C in soft margin SVM controls the balance between two competing objectives: maximizing the margin (which favors simpler models) and minimizing classification errors (which favors fitting the training data). Large C values lead to lower bias but higher variance (focusing more on fitting training data), while small C values lead to higher bias but lower variance (focusing more on maximizing margin).'), Question(question='What is the key advantage of using kernel functions in SVM instead of explicit feature mapping?', answer='Kernels allow computing inner products in high-dimensional feature spaces without explicitly representing the transformed features', distractors=['Kernels always guarantee that the optimization problem will be convex', 'Kernels eliminate the need for regularization in the SVM formulation', 'Kernels automatically select the most relevant features from the input data'], explanation='The kernel trick allows SVM to implicitly work in high-dimensional (even infinite-dimensional) feature spaces by computing inner products directly through kernel functions, without having to explicitly compute and store the transformed feature vectors φ(x). This makes the computation much more efficient than explicit feature mapping, which could result in very high-dimensional representations that are computationally expensive to handle.'), Question(question='Which of the following conditions must a function k(x,z) satisfy to be a valid kernel function?', answer='It must be symmetric and the corresponding Gram matrix must be positive semi-definite', distractors=['It must be differentiable and have a bounded range between 0 and 1', 'It must be a linear combination of polynomial and exponential functions', 'It must satisfy the triangle inequality and be translation invariant'], explanation=\"For a function to be a valid kernel, it must satisfy Mercer's conditions: (1) it must be symmetric, i.e., k(x,z) = k(z,x), and (2) the Gram matrix K defined by K_ij = k(x_i, x_j) must be positive semi-definite. These conditions ensure that the kernel corresponds to an inner product in some feature space, which is essential for the mathematical foundations of kernel methods.\"), Question(question='Comparing SVM with the previously studied Naïve Bayes classifier, what is a fundamental difference in their approach to classification?', answer='SVM is a discriminative model that directly learns the decision boundary, while Naïve Bayes is a generative model that learns class probability distributions', distractors=['SVM can only handle binary classification while Naïve Bayes naturally handles multi-class problems', 'SVM requires continuous features while Naïve Bayes only works with discrete categorical features', 'SVM uses maximum likelihood estimation while Naïve Bayes uses gradient descent optimization'], explanation=\"SVM is a discriminative model that directly learns the decision boundary (hyperplane) separating different classes, focusing on the boundary region. In contrast, Naïve Bayes (from previous lectures) is a generative model that learns the probability distributions P(X|Y) for each class and uses Bayes' theorem to make predictions. This fundamental difference affects how they handle data and make predictions.\"), Question(question='In the dual formulation of SVM, what are support vectors?', answer='Training examples with non-zero Lagrange multipliers that lie on or within the margin boundaries', distractors=['All training examples that are correctly classified by the learned hyperplane', 'The eigenvectors of the kernel matrix used in the optimization problem', 'Training examples that have the highest prediction confidence scores'], explanation=\"Support vectors are the training examples that have non-zero Lagrange multipliers (α_i > 0) in the dual formulation. These are the critical training points that lie either exactly on the margin boundary or within the margin (for soft margin SVM). Only these points influence the final decision boundary, making SVM a sparse method where many training examples (those with α_i = 0) don't affect the final model.\"), Question(question='For multi-class SVM classification, what is the main advantage of the one-against-one approach compared to one-against-rest?', answer='One-against-one is typically faster to train because each binary classifier uses fewer training examples', distractors=['One-against-one always produces higher classification accuracy than one-against-rest', 'One-against-one requires fewer binary classifiers to be trained overall', 'One-against-one can handle imbalanced datasets better than one-against-rest'], explanation='The one-against-one approach trains k(k-1)/2 binary classifiers, each using only 2N/k training examples on average (where N is total training size and k is number of classes). Although this creates more classifiers, each individual classifier is trained on a much smaller dataset, making the overall training process faster. The lecture notes that while accuracy is similar between approaches, one-against-one is fastest for training.'), Question(question='What is the main computational challenge of kernel SVM compared to linear classifiers like logistic regression?', answer='Kernel SVM training has at least O(N²) time complexity while linear classifiers can be trained in O(N) time', distractors=['Kernel SVM requires storing the entire training dataset in memory during prediction', 'Kernel SVM cannot be parallelized across multiple processors or cores', 'Kernel SVM optimization is non-convex and may get stuck in local minima'], explanation=\"The lecture highlights the 'curse of kernalization' - while kernel SVM provides powerful nonlinear classification capabilities, it comes with significant computational cost. Training kernel SVM typically requires at least O(N²) time complexity (even with efficient solvers like SMO), whereas linear classifiers can often be trained in linear time O(N). This makes kernel SVM challenging to scale to very large datasets, which is why kernel approximation methods are sometimes used.\")]), completion_tokens=1795)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from base import BaseModel \n",
    "from lite_llm import LiteLLMInput\n",
    "from lite_llm import LiteLLMSetting\n",
    "from lite_llm import LiteLLMService\n",
    "from lite_llm import Role \n",
    "from lite_llm import CompletionMessage\n",
    "from pydantic import Field\n",
    "import base64\n",
    "from pydantic import HttpUrl, SecretStr\n",
    "from generation.shared.utils import get_previous_lectures\n",
    "from storage.minio import MinioInput, MinioService, MinioSetting\n",
    "\n",
    "minio_setting = MinioSetting(\n",
    "    endpoint=\"localhost:9000\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin123\",\n",
    "    secure=False,\n",
    ")\n",
    "        \n",
    "minio_service = MinioService(settings=minio_setting)\n",
    "\n",
    "litellm_setting = LiteLLMSetting(\n",
    "    url=HttpUrl(\"http://localhost:9510\"),\n",
    "    token=SecretStr(\"abc123\"),\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    frequency_penalty=0.0,\n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_completion_tokens=10000,\n",
    "    dimension=1536,\n",
    "    embedding_model=\"gemini-embedding\"\n",
    ")\n",
    "\n",
    "litellm_service = LiteLLMService(litellm_setting=litellm_setting)\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str = Field(..., description=\"The quiz question\")\n",
    "    answer: str = Field(..., description=\"The correct answer to the quiz question\")\n",
    "    distractors: list[str] = Field(..., description=\"A list of incorrect answers to the quiz question\")\n",
    "    explanation: str = Field(..., description=\"An explanation of the correct answer\")\n",
    "    \n",
    "class Questions(BaseModel):\n",
    "    questions: list[Question] = Field(..., description=\"A list of quiz questions\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert educational content creator and quiz designer. Your task is to create comprehensive and challenging quiz questions based on the provided PDF lecture material and summaries from previous lectures.\n",
    "\n",
    "Guidelines for creating quiz questions:\n",
    "\n",
    "1. **Content Coverage**: \n",
    "   - Focus primarily on the current PDF lecture content (80% of questions)\n",
    "   - Include some questions that connect current content with previous lectures (20% of questions)\n",
    "   - Cover key concepts, definitions, algorithms, formulas, and practical applications\n",
    "\n",
    "2. **Question Types and Difficulty**:\n",
    "   - Create questions of varying difficulty levels (30% easy, 50% medium, 20% hard)\n",
    "   - Include conceptual understanding questions, not just memorization\n",
    "   - Add application-based questions that test practical knowledge\n",
    "   - Include questions about advantages/disadvantages of different approaches\n",
    "\n",
    "3. **Question Structure**:\n",
    "   - Each question should have exactly 4 answer choices (1 correct + 3 distractors)\n",
    "   - Distractors should be plausible but clearly incorrect to knowledgeable students\n",
    "   - Avoid \"all of the above\" or \"none of the above\" options\n",
    "   - Make questions clear and unambiguous\n",
    "\n",
    "4. **Answer Explanations**:\n",
    "   - Provide detailed explanations for why the correct answer is right\n",
    "   - Briefly explain why the distractors are incorrect when helpful\n",
    "   - Include relevant formulas, concepts, or references to lecture content\n",
    "\n",
    "5. **Content Integration**:\n",
    "   - When using previous lecture summaries, create questions that show progression of learning\n",
    "   - Highlight connections between current and previous topics\n",
    "   - Ensure questions test understanding rather than rote memorization\n",
    "\n",
    "6. **Quality Standards**:\n",
    "   - Questions should be appropriate for university-level students\n",
    "   - Use precise technical terminology from the field\n",
    "   - Ensure mathematical notation and formulas are accurate\n",
    "   - Create questions that encourage critical thinking\n",
    "\n",
    "Generate 8 high-quality quiz questions following these guidelines.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Here is the summary from previous lectures:\n",
    "{summary}\n",
    "Please use the pdf file and the summary above to create a quiz for the students including exactly 8 questions.\n",
    "\"\"\"\n",
    "\n",
    "pdf_path = \"/home/vuiem/KLTN/test/files/Lecture 6_Classification_SVM.pdf\"\n",
    "\n",
    "summaries = get_previous_lectures(minio_service, \"int3405\", 6)\n",
    "\n",
    "with open(pdf_path, 'rb') as pdf_file:\n",
    "    pdf_bytes = pdf_file.read()\n",
    "\n",
    "output = await litellm_service.process_async(\n",
    "    inputs=LiteLLMInput(\n",
    "        messages=[\n",
    "            CompletionMessage(\n",
    "                role=Role.SYSTEM,\n",
    "                content=system_prompt\n",
    "            ),\n",
    "            CompletionMessage(\n",
    "                role=Role.USER,\n",
    "                file_url=f\"data:application/pdf;base64,{base64.b64encode(pdf_bytes).decode('utf-8')}\"\n",
    "            ),\n",
    "            CompletionMessage(\n",
    "                role=Role.USER,\n",
    "                content=user_prompt.format(summary=\"\\n\".join(summaries))\n",
    "            )\n",
    "        ],\n",
    "        response_format=Questions,\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=10000,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        n=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657de206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Question(question='What is the primary intuition behind Support Vector Machines (SVM) for finding the optimal decision boundary?', answer='Maximize the margin between the separating hyperplane and the closest data points from both classes', distractors=['Minimize the total number of misclassified training examples', 'Maximize the likelihood of the training data given the model parameters', 'Minimize the entropy of the class distributions at the decision boundary'], explanation=\"SVM's core intuition is to find the separating hyperplane that maximizes the margin - the width that the boundary could be increased by before hitting a data point. This approach aims to find the decision boundary that is as far as possible from the nearest training examples of both classes, which typically leads to better generalization.\"),\n",
       " Question(question='In the soft margin SVM formulation, what role does the regularization parameter C play?', answer='It controls the trade-off between maximizing the margin and minimizing misclassification errors', distractors=['It determines the degree of the polynomial kernel function', 'It sets the bandwidth parameter for the Gaussian RBF kernel', 'It controls the learning rate for the gradient descent optimization'], explanation='The parameter C in soft margin SVM controls the balance between two competing objectives: maximizing the margin (which favors simpler models) and minimizing classification errors (which favors fitting the training data). Large C values lead to lower bias but higher variance (focusing more on fitting training data), while small C values lead to higher bias but lower variance (focusing more on maximizing margin).'),\n",
       " Question(question='What is the key advantage of using kernel functions in SVM instead of explicit feature mapping?', answer='Kernels allow computing inner products in high-dimensional feature spaces without explicitly representing the transformed features', distractors=['Kernels always guarantee that the optimization problem will be convex', 'Kernels eliminate the need for regularization in the SVM formulation', 'Kernels automatically select the most relevant features from the input data'], explanation='The kernel trick allows SVM to implicitly work in high-dimensional (even infinite-dimensional) feature spaces by computing inner products directly through kernel functions, without having to explicitly compute and store the transformed feature vectors φ(x). This makes the computation much more efficient than explicit feature mapping, which could result in very high-dimensional representations that are computationally expensive to handle.'),\n",
       " Question(question='Which of the following conditions must a function k(x,z) satisfy to be a valid kernel function?', answer='It must be symmetric and the corresponding Gram matrix must be positive semi-definite', distractors=['It must be differentiable and have a bounded range between 0 and 1', 'It must be a linear combination of polynomial and exponential functions', 'It must satisfy the triangle inequality and be translation invariant'], explanation=\"For a function to be a valid kernel, it must satisfy Mercer's conditions: (1) it must be symmetric, i.e., k(x,z) = k(z,x), and (2) the Gram matrix K defined by K_ij = k(x_i, x_j) must be positive semi-definite. These conditions ensure that the kernel corresponds to an inner product in some feature space, which is essential for the mathematical foundations of kernel methods.\"),\n",
       " Question(question='Comparing SVM with the previously studied Naïve Bayes classifier, what is a fundamental difference in their approach to classification?', answer='SVM is a discriminative model that directly learns the decision boundary, while Naïve Bayes is a generative model that learns class probability distributions', distractors=['SVM can only handle binary classification while Naïve Bayes naturally handles multi-class problems', 'SVM requires continuous features while Naïve Bayes only works with discrete categorical features', 'SVM uses maximum likelihood estimation while Naïve Bayes uses gradient descent optimization'], explanation=\"SVM is a discriminative model that directly learns the decision boundary (hyperplane) separating different classes, focusing on the boundary region. In contrast, Naïve Bayes (from previous lectures) is a generative model that learns the probability distributions P(X|Y) for each class and uses Bayes' theorem to make predictions. This fundamental difference affects how they handle data and make predictions.\"),\n",
       " Question(question='In the dual formulation of SVM, what are support vectors?', answer='Training examples with non-zero Lagrange multipliers that lie on or within the margin boundaries', distractors=['All training examples that are correctly classified by the learned hyperplane', 'The eigenvectors of the kernel matrix used in the optimization problem', 'Training examples that have the highest prediction confidence scores'], explanation=\"Support vectors are the training examples that have non-zero Lagrange multipliers (α_i > 0) in the dual formulation. These are the critical training points that lie either exactly on the margin boundary or within the margin (for soft margin SVM). Only these points influence the final decision boundary, making SVM a sparse method where many training examples (those with α_i = 0) don't affect the final model.\"),\n",
       " Question(question='For multi-class SVM classification, what is the main advantage of the one-against-one approach compared to one-against-rest?', answer='One-against-one is typically faster to train because each binary classifier uses fewer training examples', distractors=['One-against-one always produces higher classification accuracy than one-against-rest', 'One-against-one requires fewer binary classifiers to be trained overall', 'One-against-one can handle imbalanced datasets better than one-against-rest'], explanation='The one-against-one approach trains k(k-1)/2 binary classifiers, each using only 2N/k training examples on average (where N is total training size and k is number of classes). Although this creates more classifiers, each individual classifier is trained on a much smaller dataset, making the overall training process faster. The lecture notes that while accuracy is similar between approaches, one-against-one is fastest for training.'),\n",
       " Question(question='What is the main computational challenge of kernel SVM compared to linear classifiers like logistic regression?', answer='Kernel SVM training has at least O(N²) time complexity while linear classifiers can be trained in O(N) time', distractors=['Kernel SVM requires storing the entire training dataset in memory during prediction', 'Kernel SVM cannot be parallelized across multiple processors or cores', 'Kernel SVM optimization is non-convex and may get stuck in local minima'], explanation=\"The lecture highlights the 'curse of kernalization' - while kernel SVM provides powerful nonlinear classification capabilities, it comes with significant computational cost. Training kernel SVM typically requires at least O(N²) time complexity (even with efficient solvers like SMO), whereas linear classifiers can often be trained in linear time O(N). This makes kernel SVM challenging to scale to very large datasets, which is why kernel approximation methods are sometimes used.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.response.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2770aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"baseline.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        output.response.model_dump(),\n",
    "        f,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe738aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quiz_generation_test_output_gemini_gpt4omini.json\", \"r\") as f:\n",
    "    quiz_generation_test_output_gemini_gpt4omini = json.load(f)\n",
    " \n",
    "formatted_questions = {\n",
    "    \"questions\": []\n",
    "}\n",
    "\n",
    "for i, question in enumerate(quiz_generation_test_output_gemini_gpt4omini[\"quiz_questions\"]):\n",
    "    formatted_questions[\"questions\"].append({\n",
    "        \"question\": question[\"question\"],\n",
    "        \"answer\": question[\"answer\"],\n",
    "        \"distractors\": question[\"distractors\"],\n",
    "        \"explanation\": question[\"explanation\"]\n",
    "    })\n",
    "\n",
    "formatted_questions\n",
    "\n",
    "with open(\"formatted_quiz_generation_test_output_gemini_gpt4omini.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        formatted_questions,\n",
    "        f,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef0c4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiteLLMOutput(response='Hello! How can I help you today?', completion_tokens=9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = await litellm_service.process_async(\n",
    "    inputs=LiteLLMInput(\n",
    "        messages=[\n",
    "            CompletionMessage(\n",
    "                role=Role.USER,\n",
    "                content=\"Hello\"\n",
    "            )\n",
    "        ],\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=10000,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        n=1,\n",
    "    )\n",
    ")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
