"""
Test file for LLM as Judge Quiz Evaluation Domain

This file tests the quiz evaluation functionality using sample quiz data
from the existing quiz generation output files.
"""

import asyncio
import json
from pathlib import Path
from typing import List

from generation.domain.llm_as_judge import (
    QuizEvaluatorService,
    QuizEvaluationInput,
    QuizEvaluationOutput,
    ComprehensiveQuizEvaluationOutput,
    EvaluationCriteria
)
from generation.shared.models import QuizQuestion, Topic
from generation.shared.settings import QuizEvaluatorSetting
from lite_llm import LiteLLMService, LiteLLMSetting


# Sample quiz data based on the quiz generation output
SAMPLE_QUIZ_QUESTIONS = [
    {
        "question": "M√¥ h√¨nh Perceptron ƒë∆∞·ª£c s·ª≠ d·ª•ng ch·ªß y·∫øu ƒë·ªÉ gi·∫£i quy·∫øt lo·∫°i b√†i to√°n n√†o?",
        "answer": "Ph√¢n lo·∫°i nh·ªã ph√¢n cho d·ªØ li·ªáu t√°ch tuy·∫øn t√≠nh",
        "distractors": [
            "T·∫•t c·∫£ c√°c lo·∫°i d·ªØ li·ªáu",
            "Ch·ªâ c√°c b√†i to√°n h·ªìi quy tuy·∫øn t√≠nh",
            "Ph√¢n lo·∫°i ƒëa l·ªõp cho d·ªØ li·ªáu phi tuy·∫øn"
        ],
        "difficulty": "Easy",
        "bloom_taxonomy_level": "Remember",
        "estimated_right_answer_rate": 0.85,
        "explanation": "Perceptron l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i tuy·∫øn t√≠nh ch·ªâ c√≥ th·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n v·ªõi d·ªØ li·ªáu t√°ch tuy·∫øn t√≠nh.",
        "topic": {
            "name": "Perceptron Model Fundamentals",
            "description": "This topic covers the basic definition of the Perceptron model, its use in linear classification, and the concept of defining a separating hyperplane."
        }
    },
    {
        "question": "M·ª•c ti√™u ch√≠nh c·ªßa Hard Margin SVM l√† g√¨?",
        "answer": "T√¨m hyperplane t√°ch v·ªõi margin t·ªëi ƒëa cho d·ªØ li·ªáu t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o",
        "distractors": [
            "Gi·∫£m thi·ªÉu s·ªë l∆∞·ª£ng support vectors",
            "T·ªëi ∆∞u h√≥a tham s·ªë regularization C",
            "X·ª≠ l√Ω d·ªØ li·ªáu nhi·ªÖu v√† overlap"
        ],
        "difficulty": "Easy", 
        "bloom_taxonomy_level": "Understand",
        "estimated_right_answer_rate": 0.80,
        "explanation": "Hard Margin SVM t√¨m hyperplane ph√¢n t√°ch t·ªëi ∆∞u b·∫±ng c√°ch t·ªëi ƒëa h√≥a margin gi·ªØa c√°c l·ªõp, y√™u c·∫ßu d·ªØ li·ªáu ph·∫£i t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o.",
        "topic": {
            "name": "Hard Margin SVM Objective",
            "description": "This topic focuses on the core objective of Hard Margin SVM and its strict requirement for linearly separable datasets."
        }
    },
    {
        "question": "Slack variables (Œæ·µ¢) trong Soft Margin SVM c√≥ m·ª•c ƒë√≠ch ch√≠nh l√† g√¨?",
        "answer": "Cho ph√©p m·ªôt s·ªë ƒëi·ªÉm b·ªã ph√¢n lo·∫°i sai trong t·∫≠p hu·∫•n luy·ªán",
        "distractors": [
            "TƒÉng t·ªëc ƒë·ªô t√≠nh to√°n c·ªßa thu·∫≠t to√°n",
            "Gi·∫£m s·ªë chi·ªÅu c·ªßa kh√¥ng gian ƒë·∫∑c tr∆∞ng",
            "C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p ki·ªÉm tra"
        ],
        "difficulty": "Easy",
        "bloom_taxonomy_level": "Remember", 
        "estimated_right_answer_rate": 0.75,
        "explanation": "Slack variables cho ph√©p SVM dung th·ª© v·ªõi c√°c ƒëi·ªÉm n·∫±m trong margin ho·∫∑c b·ªã ph√¢n lo·∫°i sai, l√†m cho m√¥ h√¨nh c√≥ th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o.",
        "topic": {
            "name": "Soft Margin SVM Slack Variables",
            "description": "This topic assesses understanding of slack variables in Soft Margin SVM and their role in allowing misclassifications."
        }
    }
]

SAMPLE_LECTURE_CONTENT = """
# Lecture 6: Support Vector Machines and Perceptron

## 1. Perceptron Model
- Perceptron l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i tuy·∫øn t√≠nh ƒë∆°n gi·∫£n
- S·ª≠ d·ª•ng ƒë·ªÉ ph√¢n lo·∫°i nh·ªã ph√¢n v·ªõi d·ªØ li·ªáu t√°ch tuy·∫øn t√≠nh
- ƒê·ªãnh nghƒ©a hyperplane ph√¢n t√°ch: w^T x + b = 0
- Ch·ªâ c√≥ th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o

## 2. Hard Margin SVM
- M·ª•c ti√™u: T√¨m hyperplane t√°ch v·ªõi margin t·ªëi ƒëa
- Y√™u c·∫ßu d·ªØ li·ªáu ph·∫£i t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o
- Kh√¥ng c√≥ ƒëi·ªÉm n√†o ƒë∆∞·ª£c ph√©p n·∫±m trong margin
- T·ªëi ∆∞u h√≥a: min ||w||¬≤ subject to y·µ¢(w^T x·µ¢ + b) ‚â• 1

## 3. Soft Margin SVM  
- Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ c·ªßa Hard Margin khi d·ªØ li·ªáu kh√¥ng t√°ch tuy·∫øn t√≠nh ho√†n h·∫£o
- S·ª≠ d·ª•ng slack variables Œæ·µ¢ ƒë·ªÉ cho ph√©p m·ªôt s·ªë ƒëi·ªÉm b·ªã ph√¢n lo·∫°i sai
- C√¢n b·∫±ng gi·ªØa t·ªëi ƒëa h√≥a margin v√† gi·∫£m thi·ªÉu l·ªói ph√¢n lo·∫°i
- Tham s·ªë C ƒëi·ªÅu khi·ªÉn m·ª©c ƒë·ªô penalty cho vi·ªác vi ph·∫°m margin

## 4. Kernel Trick
- √Ånh x·∫° d·ªØ li·ªáu v√†o kh√¥ng gian ƒë·∫∑c tr∆∞ng c√≥ chi·ªÅu cao h∆°n
- Cho ph√©p SVM x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng t√°ch tuy·∫øn t√≠nh
- C√°c kernel ph·ªï bi·∫øn: Linear, Polynomial, RBF (Gaussian)
- Kh√¥ng c·∫ßn t√≠nh to√°n tr·ª±c ti·∫øp trong kh√¥ng gian ƒë·∫∑c tr∆∞ng

## 5. Multi-class Classification
- SVM ban ƒë·∫ßu ch·ªâ cho ph√¢n lo·∫°i nh·ªã ph√¢n
- One-vs-Rest: k classifiers cho k classes  
- One-vs-One: k(k-1)/2 classifiers cho k classes
- Prediction d·ª±a tr√™n voting ho·∫∑c decision function values
"""


def create_quiz_questions_from_sample() -> List[QuizQuestion]:
    """Convert sample data to QuizQuestion objects"""
    quiz_questions = []
    
    for idx, sample in enumerate(SAMPLE_QUIZ_QUESTIONS):
        # Create Topic object
        topic = Topic(
            name=sample["topic"]["name"],
            description=sample["topic"]["description"], 
            difficulty=sample["difficulty"],
            bloom_taxonomy_level=sample["bloom_taxonomy_level"],
            estimated_right_answer_rate=sample["estimated_right_answer_rate"]
        )
        
        # Create QuizQuestion object
        quiz_question = QuizQuestion(
            question=sample["question"],
            answer=sample["answer"], 
            distractors=sample["distractors"],
            explanation=sample["explanation"],
            topic=topic,
            week_number=6,
            course_code="int3405"
        )
        
        quiz_questions.append(quiz_question)
    
    return quiz_questions


async def test_basic_quiz_evaluation():
    """Test basic quiz evaluation functionality"""
    print("üß™ Testing Basic Quiz Evaluation...")
    
    # Setup LiteLLM service
    litellm_setting = LiteLLMSetting(
        base_url="https://api.openai.com/v1",
        api_key="your-api-key-here",  # Replace with actual API key
        model="gpt-4o-mini"
    )
    litellm_service = LiteLLMService(litellm_setting)
    
    # Setup evaluator service
    evaluator_setting = QuizEvaluatorSetting()
    evaluator_service = QuizEvaluatorService(litellm_service)
    
    # Create quiz questions
    quiz_questions = create_quiz_questions_from_sample()
    
    # Create evaluation input
    evaluation_input = QuizEvaluationInput(
        quiz_questions=quiz_questions,
        lecture_content=SAMPLE_LECTURE_CONTENT,
        course_code="int3405",
        week_number=6,
        evaluation_criteria=list(EvaluationCriteria)
    )
    
    try:
        # Perform evaluation
        print("üìä Evaluating quiz questions...")
        evaluation_result = await evaluator_service.evaluate_quiz(evaluation_input)
        
        # Display results
        print(f"\n‚úÖ Evaluation Complete!")
        print(f"üìà Total Score: {evaluation_result.total_score}/100")
        print(f"üéØ Grade: {evaluation_result.grade}")
        print(f"üí° Recommendation: {evaluation_result.recommendation}")
        
        print(f"\nüìã Criteria Breakdown:")
        for criteria_score in evaluation_result.criteria_scores:
            print(f"  ‚Ä¢ {criteria_score.criteria.value}: {criteria_score.score}/{criteria_score.max_score}")
        
        print(f"\nüí¨ Overall Feedback:")
        print(f"  {evaluation_result.overall_feedback[:200]}...")
        
        if evaluation_result.major_strengths:
            print(f"\n‚ú® Major Strengths:")
            for strength in evaluation_result.major_strengths[:3]:
                print(f"  ‚Ä¢ {strength}")
        
        if evaluation_result.major_weaknesses:
            print(f"\n‚ö†Ô∏è  Major Weaknesses:")
            for weakness in evaluation_result.major_weaknesses[:3]:
                print(f"  ‚Ä¢ {weakness}")
        
        return evaluation_result
        
    except Exception as e:
        print(f"‚ùå Error during evaluation: {str(e)}")
        return None


async def test_comprehensive_quiz_evaluation():
    """Test comprehensive quiz evaluation with detailed analysis"""
    print("\nüî¨ Testing Comprehensive Quiz Evaluation...")
    
    # Setup services (reuse from basic test)
    litellm_setting = LiteLLMSetting(
        base_url="https://api.openai.com/v1", 
        api_key="your-api-key-here",  # Replace with actual API key
        model="gpt-4o-mini"
    )
    litellm_service = LiteLLMService(litellm_setting)
    evaluator_service = QuizEvaluatorService(litellm_service)
    
    # Create evaluation input
    quiz_questions = create_quiz_questions_from_sample()
    evaluation_input = QuizEvaluationInput(
        quiz_questions=quiz_questions,
        lecture_content=SAMPLE_LECTURE_CONTENT,
        course_code="int3405", 
        week_number=6
    )
    
    try:
        # Perform comprehensive evaluation
        print("üîç Running comprehensive analysis...")
        comprehensive_result = await evaluator_service.evaluate_quiz_comprehensive(evaluation_input)
        
        # Display comprehensive results
        print(f"\n‚úÖ Comprehensive Evaluation Complete!")
        print(f"üìä Content Coverage: {comprehensive_result.metrics.content_coverage_percentage:.1f}%")
        print(f"üìö Questions Evaluated: {comprehensive_result.metrics.total_questions_evaluated}")
        print(f"üéØ Avg Estimated Accuracy: {comprehensive_result.metrics.average_estimated_accuracy:.2f}")
        
        print(f"\nüìà Difficulty Distribution:")
        for difficulty, count in comprehensive_result.metrics.question_difficulty_distribution.items():
            print(f"  ‚Ä¢ {difficulty}: {count} questions")
        
        print(f"\nüß† Bloom's Taxonomy Distribution:")
        for bloom_level, count in comprehensive_result.metrics.bloom_taxonomy_distribution.items():
            print(f"  ‚Ä¢ {bloom_level}: {count} questions")
        
        if comprehensive_result.content_gaps:
            print(f"\nüîç Content Gaps Identified:")
            for gap in comprehensive_result.content_gaps[:3]:
                print(f"  ‚Ä¢ {gap}")
        
        print(f"\nüìù Individual Question Evaluations:")
        for i, q_eval in enumerate(comprehensive_result.question_evaluations[:2]):
            print(f"  Question {i+1}:")
            print(f"    Content Alignment: {q_eval.content_alignment_score}/10")
            print(f"    Difficulty: {q_eval.difficulty_appropriateness_score}/10")
            print(f"    Clarity: {q_eval.clarity_score}/10")
            print(f"    Pedagogical Value: {q_eval.pedagogical_value_score}/10")
        
        return comprehensive_result
        
    except Exception as e:
        print(f"‚ùå Error during comprehensive evaluation: {str(e)}")
        return None


async def test_evaluation_with_poor_quiz():
    """Test evaluation with a deliberately poor-quality quiz to test detection"""
    print("\nüö® Testing Evaluation with Poor Quality Quiz...")
    
    # Create poor quality quiz questions
    poor_quiz_questions = [
        QuizQuestion(
            question="What is AI?",  # Too vague and not from lecture content
            answer="Artificial Intelligence",  # Not from lecture
            distractors=["Machine Learning", "Deep Learning", "Computer Vision"],
            explanation="AI is artificial intelligence.",  # Poor explanation
            topic=Topic(
                name="AI Basics",  # Not from lecture content
                description="Basic AI concepts",
                difficulty="Easy",
                bloom_taxonomy_level="Remember", 
                estimated_right_answer_rate=0.9
            ),
            week_number=6,
            course_code="int3405"
        )
    ]
    
    # Setup services
    litellm_setting = LiteLLMSetting(
        base_url="https://api.openai.com/v1",
        api_key="your-api-key-here",  # Replace with actual API key
        model="gpt-4o-mini"
    )
    litellm_service = LiteLLMService(litellm_setting)
    evaluator_service = QuizEvaluatorService(litellm_service)
    
    # Create evaluation input
    evaluation_input = QuizEvaluationInput(
        quiz_questions=poor_quiz_questions,
        lecture_content=SAMPLE_LECTURE_CONTENT,
        course_code="int3405",
        week_number=6
    )
    
    try:
        # Perform evaluation
        print("üîç Evaluating poor quality quiz...")
        evaluation_result = await evaluator_service.evaluate_quiz(evaluation_input)
        
        # Display results
        print(f"\n‚úÖ Poor Quiz Evaluation Complete!")
        print(f"üìâ Total Score: {evaluation_result.total_score}/100 (Should be low)")
        print(f"‚ùå Grade: {evaluation_result.grade} (Should be Poor/Needs Improvement)")
        print(f"üö´ Recommendation: {evaluation_result.recommendation} (Should be Reject/Revise)")
        
        print(f"\n‚ö†Ô∏è  Expected Issues Detected:")
        for criteria_score in evaluation_result.criteria_scores:
            if criteria_score.score < 60:
                print(f"  ‚Ä¢ Low {criteria_score.criteria.value}: {criteria_score.score}/{criteria_score.max_score}")
        
        return evaluation_result
        
    except Exception as e:
        print(f"‚ùå Error during poor quiz evaluation: {str(e)}")
        return None


async def save_evaluation_results(evaluation_result: QuizEvaluationOutput, filename: str):
    """Save evaluation results to JSON file"""
    try:
        output_data = {
            "evaluation_summary": {
                "total_score": evaluation_result.total_score,
                "grade": evaluation_result.grade,
                "recommendation": evaluation_result.recommendation,
                "course_code": evaluation_result.course_code,
                "week_number": evaluation_result.week_number
            },
            "criteria_scores": [
                {
                    "criteria": score.criteria.value,
                    "score": score.score,
                    "max_score": score.max_score,
                    "feedback": score.feedback,
                    "strengths": score.strengths,
                    "weaknesses": score.weaknesses,
                    "suggestions": score.suggestions
                }
                for score in evaluation_result.criteria_scores
            ],
            "overall_feedback": evaluation_result.overall_feedback,
            "major_strengths": evaluation_result.major_strengths,
            "major_weaknesses": evaluation_result.major_weaknesses,
            "priority_improvements": evaluation_result.priority_improvements
        }
        
        # Save to file
        output_path = Path(__file__).parent / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Results saved to: {output_path}")
        
    except Exception as e:
        print(f"‚ùå Error saving results: {str(e)}")


async def main():
    """Main test function"""
    print("üöÄ Starting LLM as Judge Quiz Evaluation Tests")
    print("=" * 60)
    
    # Test 1: Basic evaluation
    basic_result = await test_basic_quiz_evaluation()
    if basic_result:
        await save_evaluation_results(basic_result, "basic_evaluation_result.json")
    
    # Test 2: Comprehensive evaluation  
    comprehensive_result = await test_comprehensive_quiz_evaluation()
    if comprehensive_result:
        await save_evaluation_results(comprehensive_result, "comprehensive_evaluation_result.json")
    
    # Test 3: Poor quiz evaluation
    poor_result = await test_evaluation_with_poor_quiz()
    if poor_result:
        await save_evaluation_results(poor_result, "poor_quiz_evaluation_result.json")
    
    print("\n" + "=" * 60)
    print("üéâ All tests completed! Check the generated JSON files for detailed results.")


if __name__ == "__main__":
    # Note: To run this test, you need to:
    # 1. Install required dependencies (lite_llm, etc.)
    # 2. Set up your OpenAI API key in the LiteLLMSetting
    # 3. Run: python test_llm_as_judge.py
    
    print("‚ö†Ô∏è  Before running:")
    print("1. Make sure to set your OpenAI API key in the LiteLLMSetting")
    print("2. Install required dependencies: pip install openai litellm")
    print("3. Run with: python test_llm_as_judge.py")
    print()
    
    # Uncomment the next line to run the tests
    # asyncio.run(main())